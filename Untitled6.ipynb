{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPM28EwFbknDQcg1Agj/pJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/i3wm1/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install gensim keras-tuner imbalanced-learn Sastrawi sentencepiece seqeval ipywidgets tensorflow==2.15.0 tensorflow-addons\n",
        "\n",
        "# Download FastText\n",
        "!wget -O id.tar.gz \"https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\"\n",
        "!tar -xzf id.tar.gz\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN1hitPCdx7M",
        "outputId": "7d48437e-1dad-4fde-d341-85dd3d03b015"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.68.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=656013c855c2df504809289b05e2679b4a43679e1930738e7623087ffd53ba50\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: Sastrawi, kt-legacy, wrapt, typeguard, tensorflow-estimator, ml-dtypes, keras, jedi, tensorflow-addons, keras-tuner, seqeval, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.1\n",
            "    Uninstalling typeguard-4.4.1:\n",
            "      Successfully uninstalled typeguard-4.4.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\n",
            "tensorstore 0.1.69 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Sastrawi-1.0.1 jedi-0.19.2 keras-2.15.0 keras-tuner-1.4.7 kt-legacy-1.0.5 ml-dtypes-0.2.0 seqeval-1.2.2 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-addons-0.23.0 tensorflow-estimator-2.15.0 typeguard-2.13.3 wrapt-1.14.1\n",
            "--2024-12-11 06:48:04--  https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com/cd/0/inline/CgCXN8-3n1XXSWrK8TNkBUrsfYGBXIcS_c-Vj-3956h4aCh3H8j5PpPrB0FCRuk8zi9OR6lStlLyZBYfyYN3cLMVLCgjjCQI0-m2czP361NbhzNZNR5Pt0u5NALcJ2QoXlo/file?dl=1# [following]\n",
            "--2024-12-11 06:48:05--  https://uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com/cd/0/inline/CgCXN8-3n1XXSWrK8TNkBUrsfYGBXIcS_c-Vj-3956h4aCh3H8j5PpPrB0FCRuk8zi9OR6lStlLyZBYfyYN3cLMVLCgjjCQI0-m2czP361NbhzNZNR5Pt0u5NALcJ2QoXlo/file?dl=1\n",
            "Resolving uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com (uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com (uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CgAUZ77eiA14m7G1xRqdeENY1yfv2xhrXN-b4jJeoZch0JGge9_Uk9WHabwpvdy1hDGv61vz5esYnSEbEg9cD8CmyIKPBsSVSdXBYxURacW7Gfswd_1kzhoPb-ydO79ZAr_jAbG-z73hDOExitO-ql8zy-TVO0WdrwFJHmN345ypuiL0ashYrY7EXCqruuqzbt7rz_NoVWfLn2VsUlQIYTPdIvdUfOrk5QJmZIa1dpuijpL3eFMdblePEAx049vsgOb1zhlhD36pvKa4rYDjkQ1HCioN2XhMJfRGIbuyYArJEZg3g8oBqShx6m-0ZqCvoCMx6eTF1O0hcGCVW8aEdHrJc2x6B30payeih7LfSBXsVQ/file?dl=1 [following]\n",
            "--2024-12-11 06:48:05--  https://uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com/cd/0/inline2/CgAUZ77eiA14m7G1xRqdeENY1yfv2xhrXN-b4jJeoZch0JGge9_Uk9WHabwpvdy1hDGv61vz5esYnSEbEg9cD8CmyIKPBsSVSdXBYxURacW7Gfswd_1kzhoPb-ydO79ZAr_jAbG-z73hDOExitO-ql8zy-TVO0WdrwFJHmN345ypuiL0ashYrY7EXCqruuqzbt7rz_NoVWfLn2VsUlQIYTPdIvdUfOrk5QJmZIa1dpuijpL3eFMdblePEAx049vsgOb1zhlhD36pvKa4rYDjkQ1HCioN2XhMJfRGIbuyYArJEZg3g8oBqShx6m-0ZqCvoCMx6eTF1O0hcGCVW8aEdHrJc2x6B30payeih7LfSBXsVQ/file?dl=1\n",
            "Reusing existing connection to uc7afb7d2db72850ef080dee4d8e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2333351997 (2.2G) [application/binary]\n",
            "Saving to: ‘id.tar.gz’\n",
            "\n",
            "id.tar.gz           100%[===================>]   2.17G  57.8MB/s    in 39s     \n",
            "\n",
            "2024-12-11 06:48:45 (56.5 MB/s) - ‘id.tar.gz’ saved [2333351997/2333351997]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)\n",
        "    print(\"FastText 'id.vec' berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat 'id.vec': {e}\")\n",
        "    raise ValueError(\"Gagal memuat FastText.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SJ3zJM5d3TO",
        "outputId": "427252f3-e555-4e89-cd20-971cf8487e87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText 'id.vec' berhasil dimuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'data2.json'\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    conversations = json.load(f)\n",
        "\n",
        "def char_offset_to_token_labels(utterance, entities, tokenizer=lambda x: x.split()):\n",
        "    tokens = tokenizer(utterance)\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "    char_pos = 0\n",
        "    token_ranges = []\n",
        "    for t in tokens:\n",
        "        start_pos = char_pos\n",
        "        end_pos = start_pos + len(t)\n",
        "        token_ranges.append((start_pos, end_pos))\n",
        "        char_pos = end_pos + 1\n",
        "    for ent in entities:\n",
        "        ent_start = ent['start']\n",
        "        ent_end = ent['end']\n",
        "        ent_type = ent['entity'].upper() # 'ANIMAL', 'LOCATION', 'SYMPTOM', ...\n",
        "        ent_token_positions = []\n",
        "        for i, (ts, te) in enumerate(token_ranges):\n",
        "            if not (te <= ent_start or ts >= ent_end):\n",
        "                ent_token_positions.append(i)\n",
        "        if len(ent_token_positions) > 0:\n",
        "            labels[ent_token_positions[0]] = \"B-\"+ent_type\n",
        "            for p in ent_token_positions[1:]:\n",
        "                labels[p] = \"I-\"+ent_type\n",
        "    return tokens, labels\n",
        "\n",
        "user_utterances = []\n",
        "intents = []\n",
        "entity_labels = []\n",
        "\n",
        "for conv in conversations:\n",
        "    for turn in conv[\"turns\"]:\n",
        "        if turn[\"speaker\"] == \"user\":\n",
        "            utt = turn[\"utterance\"]\n",
        "            ents = turn.get(\"entities\", [])\n",
        "            intent = turn.get(\"intent\", \"None\")\n",
        "            tokens, ner_tags = char_offset_to_token_labels(utt, ents)\n",
        "            user_utterances.append(tokens)\n",
        "            intents.append(intent)\n",
        "            entity_labels.append(ner_tags)"
      ],
      "metadata": {
        "id": "iGsD0UqTd5k2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing untuk Intent (dengan stemming)\n",
        "with open('stopword_list_tala.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "stop_words = set(word.strip().lower() for word in stop_words)\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_text_intent(text):\n",
        "    text = clean_text(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocessing untuk NER (tanpa stemming)\n",
        "def preprocess_text_ner(text):\n",
        "    text = clean_text(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "utterances_joined = [' '.join(utt) for utt in user_utterances]\n",
        "utterances_clean_intent = [preprocess_text_intent(u) for u in utterances_joined]\n",
        "utterances_clean_ner = [preprocess_text_ner(u) for u in utterances_joined]\n",
        "\n",
        "df_data = pd.DataFrame({\n",
        "    'utterances': utterances_joined,\n",
        "    'intent': intents,\n",
        "    'entities': entity_labels,\n",
        "    'utterances_clean_intent': utterances_clean_intent,\n",
        "    'utterances_clean_ner': utterances_clean_ner\n",
        "})\n",
        "\n",
        "label_encoder_intent = LabelEncoder()\n",
        "df_data['intent_label'] = label_encoder_intent.fit_transform(df_data['intent'])\n",
        "\n",
        "# Oversampling untuk menangani imbalance\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df_data.index.values.reshape(-1, 1)\n",
        "y = df_data['intent_label']\n",
        "X_ros, y_ros = ros.fit_resample(X, y)\n",
        "df_balanced = df_data.loc[X_ros.flatten()].reset_index(drop=True)\n",
        "df_balanced['intent_label'] = y_ros\n",
        "df_balanced['intent'] = label_encoder_intent.inverse_transform(df_balanced['intent_label'])"
      ],
      "metadata": {
        "id": "A7ss9xINeAvN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data untuk Intent\n",
        "texts_intent = df_balanced['utterances_clean_intent'].tolist()\n",
        "labels_intent = df_balanced['intent_label'].tolist()\n",
        "\n",
        "# Data untuk NER\n",
        "texts_ner = df_balanced['utterances_clean_ner'].tolist()\n",
        "labels_ner = df_balanced['entities'].tolist()\n",
        "\n",
        "# Split untuk Intent\n",
        "train_texts_intent, val_texts_intent, train_labels_intent, val_labels_intent = train_test_split(\n",
        "    texts_intent,\n",
        "    labels_intent,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels_intent\n",
        ")\n",
        "\n",
        "# Split untuk NER\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    texts_ner,\n",
        "    labels_ner,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels_intent  # Pastikan stratifikasi konsisten dengan Intent\n",
        ")\n",
        "\n",
        "# Tokenizer untuk Intent dan NER\n",
        "tokenizer_intent = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer_intent.fit_on_texts(train_texts_intent)\n",
        "word_index_intent = tokenizer_intent.word_index\n",
        "vocab_size_intent = len(word_index_intent) + 1\n",
        "\n",
        "tokenizer_ner = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer_ner.fit_on_texts(train_texts_ner)\n",
        "word_index_ner = tokenizer_ner.word_index\n",
        "vocab_size_ner = len(word_index_ner) + 1\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences_intent = tokenizer_intent.texts_to_sequences(train_texts_intent)\n",
        "val_sequences_intent = tokenizer_intent.texts_to_sequences(val_texts_intent)\n",
        "train_sequences_ner = tokenizer_ner.texts_to_sequences(train_texts_ner)\n",
        "val_sequences_ner = tokenizer_ner.texts_to_sequences(val_texts_ner)\n",
        "\n",
        "# Tentukan max_seq_length\n",
        "max_seq_length_intent = max(max(len(seq) for seq in train_sequences_intent), max(len(seq) for seq in val_sequences_intent))\n",
        "max_seq_length_ner = max(max(len(seq) for seq in train_sequences_ner), max(len(seq) for seq in val_sequences_ner))\n",
        "max_seq_length = max(max_seq_length_intent, max_seq_length_ner)  # Gunakan yang terbesar\n",
        "\n",
        "# Pad sequences\n",
        "train_padded_intent = tf.keras.preprocessing.sequence.pad_sequences(train_sequences_intent, maxlen=max_seq_length, padding='post')\n",
        "val_padded_intent = tf.keras.preprocessing.sequence.pad_sequences(val_sequences_intent, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "train_padded_ner = tf.keras.preprocessing.sequence.pad_sequences(train_sequences_ner, maxlen=max_seq_length, padding='post')\n",
        "val_padded_ner = tf.keras.preprocessing.sequence.pad_sequences(val_sequences_ner, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Encode labels untuk Intent\n",
        "num_classes_intent = len(label_encoder_intent.classes_)\n",
        "train_labels_cat_intent = tf.keras.utils.to_categorical(train_labels_intent, num_classes=num_classes_intent)\n",
        "val_labels_cat_intent = tf.keras.utils.to_categorical(val_labels_intent, num_classes=num_classes_intent)\n",
        "\n",
        "# Encode labels untuk NER\n",
        "all_labels = set()\n",
        "for tags in labels_ner:\n",
        "    for t in tags:\n",
        "        all_labels.add(t)\n",
        "all_labels = sorted(list(all_labels))\n",
        "\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(all_labels)}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "def encode_ner_labels(labels, max_len):\n",
        "    encoded = []\n",
        "    for lab_seq in labels:\n",
        "        seq_ids = [ner_label_encoder[l] for l in lab_seq]\n",
        "        seq_ids = seq_ids[:max_len] + [ner_label_encoder['O']]*(max_len-len(seq_ids))\n",
        "        encoded.append(seq_ids)\n",
        "    return np.array(encoded)\n",
        "\n",
        "Y_ner_train = encode_ner_labels(train_labels_ner, max_seq_length)\n",
        "Y_ner_val = encode_ner_labels(val_labels_ner, max_seq_length)\n",
        "\n",
        "# One-hot encoding untuk NER\n",
        "num_classes_ner = len(ner_label_encoder)\n",
        "Y_ner_train_cat = tf.keras.utils.to_categorical(Y_ner_train, num_classes=num_classes_ner)\n",
        "Y_ner_val_cat = tf.keras.utils.to_categorical(Y_ner_val, num_classes=num_classes_ner)"
      ],
      "metadata": {
        "id": "zJrUfyKBeCwt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
        "                                 initializer='glorot_uniform', trainable=True)\n",
        "        self.b = self.add_weight(name='att_bias', shape=(1,),\n",
        "                                 initializer='zeros', trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = tf.squeeze(tf.tensordot(x, self.W, axes=1), axis=-1) + self.b\n",
        "        alpha = tf.nn.softmax(e)\n",
        "        alpha = tf.expand_dims(alpha, axis=-1)\n",
        "        context = x * alpha\n",
        "        return tf.reduce_sum(context, axis=1)\n",
        "\n",
        "def build_intent_model(embedding_matrix, max_seq_length, num_classes, l2_reg=1e-3):\n",
        "    inputs = tf.keras.Input(shape=(max_seq_length,), dtype='int32')\n",
        "    emb = tf.keras.layers.Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                                    output_dim=embedding_matrix.shape[1],\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=max_seq_length,\n",
        "                                    trainable=True,\n",
        "                                    mask_zero=True)(inputs)\n",
        "    # Hapus recurrent_dropout untuk menghindari peringatan cuDNN\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3))(emb)\n",
        "    att = AttentionLayer()(x)\n",
        "    dense = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(att)\n",
        "    dropout = tf.keras.layers.Dropout(0.5)(dense)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dropout)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Buat embedding matrix untuk Intent\n",
        "embedding_matrix_intent = np.zeros((vocab_size_intent, embedding_dim))\n",
        "for word, idx in tokenizer_intent.word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix_intent[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix_intent[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "# Bangun dan latih model Intent\n",
        "intent_model = build_intent_model(embedding_matrix_intent, max_seq_length, num_classes_intent)\n",
        "early_intent = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "print(\"=== Model Intent Summary ===\")\n",
        "intent_model.summary()\n",
        "\n",
        "history_intent = intent_model.fit(\n",
        "    train_padded_intent, train_labels_cat_intent,\n",
        "    validation_data=(val_padded_intent, val_labels_cat_intent),\n",
        "    epochs=30, batch_size=16,\n",
        "    callbacks=[early_intent]\n",
        ")\n",
        "\n",
        "# Simpan model Intent\n",
        "intent_model.save('app/models/model_intent.keras')\n",
        "\n",
        "# Simpan tokenizer dan label encoders\n",
        "with open('app/encoders/tokenizer_intent.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer_intent, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('app/encoders/tokenizer_ner.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer_ner, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('app/encoders/label_encoder_intent.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder_intent, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('app/encoders/ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Simpan vectorizer dan tfidf_matrix\n",
        "with open('app/data/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('app/data/tfidf_matrix.pickle', 'wb') as handle:\n",
        "    pickle.dump(tfidf_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WrdpAgjzeEvD",
        "outputId": "9698dabf-dccb-4c46-9ef6-7b0210b0ad71"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model Intent Summary ===\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 9)]               0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, 9, 300)            46800     \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirecti  (None, 9, 256)            439296    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " attention_layer_2 (Attenti  (None, 256)               257       \n",
            " onLayer)                                                        \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 519894 (1.98 MB)\n",
            "Trainable params: 519894 (1.98 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "18/18 [==============================] - 14s 219ms/step - loss: 1.6128 - accuracy: 0.4643 - val_loss: 1.2780 - val_accuracy: 0.5714\n",
            "Epoch 2/30\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 1.0647 - accuracy: 0.6929 - val_loss: 0.7714 - val_accuracy: 0.8143\n",
            "Epoch 3/30\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.6529 - accuracy: 0.8321 - val_loss: 0.4820 - val_accuracy: 0.8571\n",
            "Epoch 4/30\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.3847 - accuracy: 0.9321 - val_loss: 0.3902 - val_accuracy: 0.9000\n",
            "Epoch 5/30\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.2671 - accuracy: 0.9571 - val_loss: 0.2825 - val_accuracy: 0.9571\n",
            "Epoch 6/30\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.2053 - accuracy: 0.9786 - val_loss: 0.2507 - val_accuracy: 0.9571\n",
            "Epoch 7/30\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1691 - accuracy: 0.9857 - val_loss: 0.2288 - val_accuracy: 0.9429\n",
            "Epoch 8/30\n",
            "18/18 [==============================] - 1s 68ms/step - loss: 0.1603 - accuracy: 0.9893 - val_loss: 0.2784 - val_accuracy: 0.9429\n",
            "Epoch 9/30\n",
            "18/18 [==============================] - 2s 94ms/step - loss: 0.1701 - accuracy: 0.9821 - val_loss: 0.2809 - val_accuracy: 0.9429\n",
            "Epoch 10/30\n",
            "18/18 [==============================] - 2s 91ms/step - loss: 0.1417 - accuracy: 0.9929 - val_loss: 0.2527 - val_accuracy: 0.9429\n",
            "Epoch 11/30\n",
            "18/18 [==============================] - 1s 76ms/step - loss: 0.1391 - accuracy: 0.9929 - val_loss: 0.2173 - val_accuracy: 0.9429\n",
            "Epoch 12/30\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1172 - accuracy: 1.0000 - val_loss: 0.2223 - val_accuracy: 0.9571\n",
            "Epoch 13/30\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1088 - accuracy: 1.0000 - val_loss: 0.2091 - val_accuracy: 0.9571\n",
            "Epoch 14/30\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.9429\n",
            "Epoch 15/30\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 0.1739 - val_accuracy: 0.9429\n",
            "Epoch 16/30\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.1727 - val_accuracy: 0.9571\n",
            "Epoch 17/30\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.0878 - accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9429\n",
            "Epoch 18/30\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.0833 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9571\n",
            "Epoch 19/30\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.0792 - accuracy: 1.0000 - val_loss: 0.1832 - val_accuracy: 0.9571\n",
            "Epoch 20/30\n",
            "18/18 [==============================] - 1s 45ms/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9571\n",
            "Epoch 21/30\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.0854 - accuracy: 0.9964 - val_loss: 0.1481 - val_accuracy: 0.9714\n",
            "Epoch 22/30\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0773 - accuracy: 0.9929 - val_loss: 0.2443 - val_accuracy: 0.9571\n",
            "Epoch 23/30\n",
            "18/18 [==============================] - 2s 98ms/step - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.1358 - val_accuracy: 0.9571\n",
            "Epoch 24/30\n",
            "18/18 [==============================] - 2s 99ms/step - loss: 0.0683 - accuracy: 0.9964 - val_loss: 0.2073 - val_accuracy: 0.9429\n",
            "Epoch 25/30\n",
            "18/18 [==============================] - 2s 96ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 0.1777 - val_accuracy: 0.9857\n",
            "Epoch 26/30\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.0761 - accuracy: 0.9964 - val_loss: 0.3065 - val_accuracy: 0.9429\n",
            "Epoch 27/30\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0947 - accuracy: 0.9857 - val_loss: 0.0937 - val_accuracy: 0.9857\n",
            "Epoch 28/30\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.0664 - accuracy: 0.9929 - val_loss: 0.1733 - val_accuracy: 0.9571\n",
            "Epoch 29/30\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0632 - accuracy: 0.9964 - val_loss: 0.1563 - val_accuracy: 0.9571\n",
            "Epoch 30/30\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9429\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vectorizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-bedcc1845905>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Simpan vectorizer dan tfidf_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'app/data/vectorizer.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'app/data/tfidf_matrix.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat embedding matrix untuk NER\n",
        "embedding_matrix_ner = np.zeros((vocab_size_ner, embedding_dim))\n",
        "for word, idx in tokenizer_ner.word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix_ner[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix_ner[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "# Bangun model NER dengan CRF\n",
        "inputs_ner = tf.keras.Input(shape=(max_seq_length,), dtype='int32')\n",
        "emb_ner = tf.keras.layers.Embedding(\n",
        "    input_dim=embedding_matrix_ner.shape[0],\n",
        "    output_dim=embedding_matrix_ner.shape[1],\n",
        "    weights=[embedding_matrix_ner],\n",
        "    trainable=True,\n",
        "    mask_zero=True\n",
        ")(inputs_ner)\n",
        "# Hapus recurrent_dropout untuk menghindari peringatan cuDNN\n",
        "x_ner = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3))(emb_ner)\n",
        "x_ner = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3))(x_ner)\n",
        "dense_ner = tf.keras.layers.Dense(num_classes_ner)(x_ner)\n",
        "\n",
        "# Tambahkan CRF sebagai bagian dari model\n",
        "crf = tfa.layers.CRF(num_classes_ner)\n",
        "output_crf = crf(dense_ner)\n",
        "\n",
        "# Bangun model NER dengan CRF\n",
        "ner_model = tf.keras.Model(inputs=inputs_ner, outputs=output_crf)\n",
        "\n",
        "# Compile model NER dengan CRF\n",
        "# Gunakan CRF.loss dan CRF.accuracy sebagai fungsi loss dan metrics\n",
        "ner_model.compile(optimizer=tf.keras.optimizers.Adam(3e-5),\n",
        "                  loss=tfa.layers.CRF.loss,\n",
        "                  metrics=[tfa.layers.CRF.accuracy])\n",
        "\n",
        "print(\"=== Model NER dengan CRF Summary ===\")\n",
        "ner_model.summary()\n",
        "\n",
        "# Simpan model NER sebelum pelatihan\n",
        "ner_model.save('app/models/model_ner.keras')\n",
        "\n",
        "# Latih model NER\n",
        "early_ner = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history_ner = ner_model.fit(\n",
        "    train_padded_ner, Y_ner_train_cat,\n",
        "    validation_data=(val_padded_ner, Y_ner_val_cat),\n",
        "    epochs=10, batch_size=16,\n",
        "    callbacks=[early_ner]\n",
        ")\n",
        "\n",
        "# Simpan model NER setelah pelatihan\n",
        "ner_model.save('app/models/model_ner.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gXz5mcXWeHGE",
        "outputId": "0a1106d4-e59c-45bb-c3b0-f9bafa05b21d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'CRF' has no attribute 'loss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-468b9492b57b>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Gunakan CRF.loss dan CRF.accuracy sebagai fungsi loss dan metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m ner_model.compile(optimizer=tf.keras.optimizers.Adam(3e-5),\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   metrics=[tfa.layers.CRF.accuracy])\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'CRF' has no attribute 'loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kIJo0VV0eJmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}